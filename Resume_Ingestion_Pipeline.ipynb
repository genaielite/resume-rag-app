{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R50Y5dIlUQX",
        "outputId": "2f0f9559-238a-40da-d26b-4f948c273cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.5/80.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install -qU langchain-openai langchain langchain-community sentence-transformers unstructured[all-docs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "from unstructured.partition.docx import partition_docx\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "iaGy_eTem-f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import docx\n",
        "from unstructured.partition.docx import partition_docx\n",
        "from pathlib import Path\n",
        "\n",
        "def load_resumes(folder: str | Path, pattern: str = \"*.docx\") -> List[str]:\n",
        "    \"\"\"\n",
        "    Load all .docx files from a folder into a dict:\n",
        "      { \"<filename>\": \"<plain_text>\" }\n",
        "    \"\"\"\n",
        "    folder = Path(folder)\n",
        "    if not folder.exists():\n",
        "        raise FileNotFoundError(f\"Folder not found: {folder.resolve()}\")\n",
        "\n",
        "    file_list = []\n",
        "    for file in sorted(folder.glob(pattern)):\n",
        "        if file.is_file():\n",
        "          file_list.append(file)\n",
        "\n",
        "    return file_list"
      ],
      "metadata": {
        "id": "692GAuuXraL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = load_resumes(\"/content/resume_db\")\n",
        "file_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTe6jUwesTA-",
        "outputId": "dec44bc5-85db-40e3-eef6-dd6d073caca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/content/resume_db/Candidate 105.docx')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str(file_list[0].absolute())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Bi2ecsYSszav",
        "outputId": "542fc086-6f7a-4b42-9056-845fe73a08f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/resume_db/Candidate 105.docx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partitioning the data"
      ],
      "metadata": {
        "id": "F6krObX40NSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docx_path = str(file_list[0].absolute())\n",
        "\n",
        "elements = partition_docx(filename=docx_path,include_page_breaks=False,infer_table_structure=True)\n"
      ],
      "metadata": {
        "id": "xzC8ySNwnzrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(elements))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grz_2GmLrDtw",
        "outputId": "27a42d6d-aee7-4f83-9c7f-167d06092316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(elements[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3gChH0erOSk",
        "outputId": "ac8d7f17-90a9-4a6e-c184-9a41204dabaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'unstructured.documents.elements.ListItem'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding Resume Text"
      ],
      "metadata": {
        "id": "EUZL6CJLs7jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text = \"\"\n",
        "\n",
        "for el in elements:\n",
        "  if el.category.lower() != \"header\" and el.category.lower() != \"footer\":\n",
        "    resume_text += el.text.lower() + \"\\n\"\n"
      ],
      "metadata": {
        "id": "KwVNcaupfBh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.getsizeof(resume_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6ikhhICtuXj",
        "outputId": "92ca78c2-ec13-4d9d-e01d-8a47b6da91af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35620"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call LLM to find MetaData like - experience_years_total,skills,certifications"
      ],
      "metadata": {
        "id": "0v_vxYaQuahM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "UTPsu8wk3SfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2,max_tokens=1500)"
      ],
      "metadata": {
        "id": "JZbe5eS74tjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser"
      ],
      "metadata": {
        "id": "TjkRPzFgAWah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Act like an expert in extracting resume facts and respond in strict JSON :\n",
        "\n",
        "given resume text : {resume_text}\n",
        "\n",
        "Instructions:\n",
        "1) Derive total exact experience found in resume text in years . Use decimals if possible.\n",
        "2) Extract skills and Normalize skills to common names (map synonyms).\n",
        "3) Include certifications (exact names) if present.\n",
        "4) Extract Key Responsibilities of the candidate\n",
        "\n",
        "Below are the valid key and value of the JSON Object -\n",
        "\"experience_years_total\": number,\n",
        "\"skills\": string[],\n",
        "\"certifications\": string[],\n",
        "\"key_responsibilities\": string[]\n",
        "\n",
        "Don't include json keyword in the begining of output\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YJNj6N8HCFix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "Gg9KDcN_CR9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAG1u8NcCUDv",
        "outputId": "3c962ce3-eb77-4872-838b-fa4750ecadbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['resume_text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['resume_text'], input_types={}, partial_variables={}, template='\\nAct like an expert in extracting resume facts and respond in strict JSON :\\n\\ngiven resume text : {resume_text}\\n\\nInstructions:\\n1) Derive total experience in years from date ranges or explicit phrases. Use decimals.\\n2) Extract top 5 skills and Normalize skills to common names (map synonyms).\\n3) Include certifications (exact names) if present.\\n4) Extract Key Responsibilities of the candidate\\n\\nBelow are the valid key and value of the JSON Object - \\n\"experience_years_total\": number,\\n\"skills\": string[], \\n\"certifications\": string[],\\n\"key_responsibilities\": string[]\\n\\nDon\\'t include json keyword in the begining of output\\n\\n'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | JsonOutputParser()\n",
        "llm_response_json = chain.invoke({\"resume_text\":resume_text})\n",
        "llm_response_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrL6NL1B5Qq4",
        "outputId": "398b0c4d-5efa-4ed4-9174-2c3c741c9229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'experience_years_total': 14,\n",
              " 'skills': ['SAP BW',\n",
              "  'SAP BI',\n",
              "  'ABAP',\n",
              "  'BW/4HANA',\n",
              "  'AMDP',\n",
              "  'Native HANA',\n",
              "  'BOBJ',\n",
              "  'Power BI',\n",
              "  'HANA Views',\n",
              "  'DAX',\n",
              "  'Power Automate',\n",
              "  'BEx Analyzer',\n",
              "  'BI Portal',\n",
              "  'APDs',\n",
              "  'QlikView',\n",
              "  'S/4HANA',\n",
              "  'Rebates Management',\n",
              "  'Settlement Management',\n",
              "  'PLM',\n",
              "  'PM',\n",
              "  'OTC',\n",
              "  'CDS Views',\n",
              "  'SDA',\n",
              "  'Demantra',\n",
              "  'ODP Framework',\n",
              "  'ServiceNow',\n",
              "  'JIRA',\n",
              "  'Remedy',\n",
              "  'Heat',\n",
              "  'SAP BO Security',\n",
              "  'Broadcasting',\n",
              "  'SAP BOBJ WebI',\n",
              "  'HANA Remediation',\n",
              "  'Ticketing Tools'],\n",
              " 'certifications': [],\n",
              " 'key_responsibilities': ['Solution architect in analytics practice',\n",
              "  'Solutioned webi to powerbi conversion project',\n",
              "  'Created dataflows, datasets, reports, paginated reports',\n",
              "  'Architected Power BI dashboard design',\n",
              "  'Worked on BO 4.3 upgrade project',\n",
              "  'Worked on BW/4HANA 1.0 green field implementation',\n",
              "  'Worked on BW/4HANA 2.0 upgrade projects',\n",
              "  'Worked extensively on SAP BI in various domains',\n",
              "  'Worked with ABAP CDS views and native HANA modeling',\n",
              "  'Provided key user training and proposed system improvements',\n",
              "  'Involved in performance tuning techniques',\n",
              "  'Worked on SAP BO security setup and broadcasting',\n",
              "  'Involved in HANA upgradation project',\n",
              "  'Worked through different ticketing tools',\n",
              "  'Involved in presale for projects',\n",
              "  'Part of pioneer green field implementation of S/4HANA 1610 system',\n",
              "  'Enabled new settlement management document settings in S/4',\n",
              "  'Designed PLM and PM BI architecture',\n",
              "  'Worked on OTC push reports',\n",
              "  'Leveraged CDS views functionality',\n",
              "  'Created procedures for third-party data integration',\n",
              "  'Involved in data refresh activities',\n",
              "  'Managed incidents and changes through ServiceNow',\n",
              "  'Analyzed standard CDS views introduced by SAP',\n",
              "  'Solutioned PBI designs per existing data models',\n",
              "  'Created executive dashboards',\n",
              "  'Helped business with paginated reports',\n",
              "  'Involved in requirement gathering and gap analysis',\n",
              "  'Created generic extractors with views and function modules',\n",
              "  'Involved in performance tuning and cutover activities',\n",
              "  'Worked on early watch reports and implemented recommended actions',\n",
              "  'Handled tickets and change requests',\n",
              "  'Involved in debugging and improving system performance']}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response_json['key_responsibilities']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMYtkw3CE5Rf",
        "outputId": "02ef0650-ab00-494d-921b-d7e9af1432c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Solution architect in analytics practice',\n",
              " 'Solutioned webi to powerbi conversion project',\n",
              " 'Created dataflows, datasets, reports, paginated reports',\n",
              " 'Architected Power BI dashboard design',\n",
              " 'Worked on BO 4.3 upgrade project',\n",
              " 'Worked on BW/4HANA 1.0 green field implementation',\n",
              " 'Worked on BW/4HANA 2.0 upgrade projects',\n",
              " 'Worked extensively on SAP BI in various domains',\n",
              " 'Worked with ABAP CDS views and native HANA modeling',\n",
              " 'Provided key user training and proposed system improvements',\n",
              " 'Involved in performance tuning techniques',\n",
              " 'Worked on SAP BO security setup and broadcasting',\n",
              " 'Involved in HANA upgradation project',\n",
              " 'Worked through different ticketing tools',\n",
              " 'Involved in presale for projects',\n",
              " 'Part of pioneer green field implementation of S/4HANA 1610 system',\n",
              " 'Enabled new settlement management document settings in S/4',\n",
              " 'Designed PLM and PM BI architecture',\n",
              " 'Worked on OTC push reports',\n",
              " 'Leveraged CDS views functionality',\n",
              " 'Created procedures for third-party data integration',\n",
              " 'Involved in data refresh activities',\n",
              " 'Managed incidents and changes through ServiceNow',\n",
              " 'Analyzed standard CDS views introduced by SAP',\n",
              " 'Solutioned PBI designs per existing data models',\n",
              " 'Created executive dashboards',\n",
              " 'Helped business with paginated reports',\n",
              " 'Involved in requirement gathering and gap analysis',\n",
              " 'Created generic extractors with views and function modules',\n",
              " 'Involved in performance tuning and cutover activities',\n",
              " 'Worked on early watch reports and implemented recommended actions',\n",
              " 'Handled tickets and change requests',\n",
              " 'Involved in debugging and improving system performance']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key_responsibilities = [item.lower() for item in llm_response_json['key_responsibilities']]\n",
        "key_responsibilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoWoYm7qoyf8",
        "outputId": "c6763c7f-a96f-4479-e1e9-15f877281e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['solution architect in analytics practice',\n",
              " 'solutioned webi to powerbi conversion project',\n",
              " 'created dataflows, datasets, reports, paginated reports',\n",
              " 'architected power bi dashboard design',\n",
              " 'worked on bo 4.3 upgrade project',\n",
              " 'worked on bw/4hana 1.0 green field implementation',\n",
              " 'worked on bw/4hana 2.0 upgrade projects',\n",
              " 'worked extensively on sap bi in various domains',\n",
              " 'worked with abap cds views and native hana modeling',\n",
              " 'provided key user training and proposed system improvements',\n",
              " 'involved in performance tuning techniques',\n",
              " 'worked on sap bo security setup and broadcasting',\n",
              " 'involved in hana upgradation project',\n",
              " 'worked through different ticketing tools',\n",
              " 'involved in presale for projects',\n",
              " 'part of pioneer green field implementation of s/4hana 1610 system',\n",
              " 'enabled new settlement management document settings in s/4',\n",
              " 'designed plm and pm bi architecture',\n",
              " 'worked on otc push reports',\n",
              " 'leveraged cds views functionality',\n",
              " 'created procedures for third-party data integration',\n",
              " 'involved in data refresh activities',\n",
              " 'managed incidents and changes through servicenow',\n",
              " 'analyzed standard cds views introduced by sap',\n",
              " 'solutioned pbi designs per existing data models',\n",
              " 'created executive dashboards',\n",
              " 'helped business with paginated reports',\n",
              " 'involved in requirement gathering and gap analysis',\n",
              " 'created generic extractors with views and function modules',\n",
              " 'involved in performance tuning and cutover activities',\n",
              " 'worked on early watch reports and implemented recommended actions',\n",
              " 'handled tickets and change requests',\n",
              " 'involved in debugging and improving system performance']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.chunking.title import chunk_by_title"
      ],
      "metadata": {
        "id": "fTkaz7s4v13D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Chunking - Find each chunks"
      ],
      "metadata": {
        "id": "jkwTZJv5u5SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "refined_elements = []\n",
        "\n",
        "for el in elements:\n",
        "  if el.category.lower() != \"header\" and el.category.lower() != \"footer\":\n",
        "    refined_elements.append(el)\n",
        "\n",
        "# Step 2: Apply chunking (chunk by title - assumes some sections have headers)\n",
        "chunks = chunk_by_title(refined_elements)\n",
        "\n",
        "\n",
        "# Optional: Convert chunks into raw text for further NLP\n",
        "# This part also needs to handle the possibility that chunks might not be a list of lists\n",
        "final_chunks_text = []\n",
        "\n",
        "final_chunks_text = [chunk.text for chunk in chunks]\n",
        "\n",
        "# Save to disk or further process\n",
        "with open(\"chunked_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for idx, text in enumerate(final_chunks_text):\n",
        "        f.write(f\"\\n--- Chunk {idx + 1} ---\\n{text}\\n\")"
      ],
      "metadata": {
        "id": "8xXCG0X9v5_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chunks_text[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "UQik5Ehy18lq",
        "outputId": "416acac3-ab63-4ce5-860b-696f1f4ecc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recently worked on BO 4.3 Upgrade project: Get the smoke test activities done on WEBI as well as CRYSTAL reports, Publications and WEBi job schedules across the landscape. Also, fixed a couple of issues faced with publication and transportation.\\n\\nWorked on traditional BW world from 2.x through BW 7.5 for different clients.\\n\\nWorked on BW/4HANA 1.0 Green field implementation and further worked on BW/4HANA 2.0 upgrade projects across the landscape. Recently, involved in 2.0 SP09 Migration.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning and pre processing pipeline"
      ],
      "metadata": {
        "id": "KZ67dVzSvFwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.cleaners.core import bytes_string_to_string\n",
        "from unstructured.cleaners.core import clean\n",
        "from unstructured.cleaners.core import clean_bullets\n",
        "from unstructured.cleaners.core import clean_dashes\n",
        "from unstructured.cleaners.core import clean_non_ascii_chars\n",
        "from unstructured.cleaners.core import clean_ordered_bullets\n",
        "from unstructured.cleaners.core import clean_trailing_punctuation\n",
        "from unstructured.cleaners.core import remove_punctuation"
      ],
      "metadata": {
        "id": "yUnQJ0IDvLIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_chunks(all_chunks):\n",
        "  cleaned_chunks = []\n",
        "  for chunk in all_chunks:\n",
        "    #refined_chunk = bytes_string_to_string(chunk, encoding=\"utf-8\")\n",
        "    #refined_chunk = clean(chunk, lowercase=True)\n",
        "    refined_chunk = chunk.lower()\n",
        "    refined_chunk = clean_bullets(refined_chunk)\n",
        "    #refined_chunk = clean_dashes(refined_chunk)\n",
        "    #refined_chunk = clean_non_ascii_chars(refined_chunk)\n",
        "    #refined_chunk = clean_ordered_bullets(refined_chunk)\n",
        "    refined_chunk = clean_trailing_punctuation(refined_chunk)\n",
        "    #refined_chunk = remove_punctuation(refined_chunk)\n",
        "    cleaned_chunks.append(refined_chunk)\n",
        "  return cleaned_chunks\n",
        "\n"
      ],
      "metadata": {
        "id": "eGMI2j-1xnm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_chunks = clean_chunks(final_chunks_text)"
      ],
      "metadata": {
        "id": "v8NOMToqzhFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, chunk in enumerate(cleaned_chunks):\n",
        "  print(\"chunk numer:: \", idx)\n",
        "  print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqHeZWgntXIv",
        "outputId": "4c134290-82a7-40b5-92ae-8321e0ed6a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunk numer::  0\n",
            "professional summary\n",
            "\n",
            "14 years of experience in sap bw/bi/abap, bw/4hana, amdp, native hana, bobj and powerbi.\n",
            "\n",
            "working as a solution architect in analytics practice.\n",
            "\n",
            "recently solutioned the webi to powerbi conversion project wherein converting all the webi reports into powerbi reports via hana views; basically, creating hana views following best practices code push down having all the calculations done at hana views then, expose them to powerbi\n",
            "chunk numer::  1\n",
            "created dataflows, datasets, reports, paginated reports consuming the sources from hana, sharepoint, and azure synapse. worked through the different dax formulas during the report design.\n",
            "\n",
            "architected the powerbi expensive dashboard design; consuming hana expensive tables into views then dashboard at powerbi end. automated the flows and datasets through power automate\n",
            "chunk numer::  2\n",
            "recently worked on bo 4.3 upgrade project: get the smoke test activities done on webi as well as crystal reports, publications and webi job schedules across the landscape. also, fixed a couple of issues faced with publication and transportation.\n",
            "\n",
            "worked on traditional bw world from 2.x through bw 7.5 for different clients.\n",
            "\n",
            "worked on bw/4hana 1.0 green field implementation and further worked on bw/4hana 2.0 upgrade projects across the landscape. recently, involved in 2.0 sp09 migration\n",
            "chunk numer::  3\n",
            "working with gyansys infotech pvt. ltd., as a solution architect in analytics practice.\n",
            "\n",
            "worked extensively on sap bi in the logistics, purchasing, materials management, inventory, copa, accounts payable, account receivables, general ledger, ppm, pm and plm.\n",
            "\n",
            "have been working through the abap cds views, adsos, cps, open ods views, query providers along with native hana modelling objects like analytical views, calculated views, table functions and store procedures\n",
            "chunk numer::  4\n",
            "done the modelling by using all the traditional sap bi objects like info object, dso, cube, multi provider, infoset. adso, cp, open ods view.\n",
            "\n",
            "created conditions, exceptions, cell definitions, ckfs, rkfs, all types of variables, rri in reporting as business required.\n",
            "\n",
            "worked with bex analyzer and bi portal too. extensively worked with apds.\n",
            "\n",
            "worked with the clients like maxxium, essent, kpn, provimi, akzonobel, hibu, ge, stanly black & decker, and delta faucet\n",
            "chunk numer::  5\n",
            "acted as spoc, incident manager and conducted weekly and monthly meetings with key members.\n",
            "\n",
            "worked in different locations in india like mumbai, bangalore, hyderabad. also, travelled netherlands and us.\n",
            "\n",
            "provided the key user training, proposed changes to improve the bw system performance and completed them successfully\n",
            "chunk numer::  6\n",
            "as an incident manager, have made sure that all the logged incidents were going well as per their priority and had the daily stand-up meetings with the team. did the follow-up on aging incidents and make sure that all the module guys were coordinating well if an incident involved cross modules.\n",
            "\n",
            "worked on various domains manufacturing, utilities, retail, chemical and pharmaceutical industries\n",
            "chunk numer::  7\n",
            "involved in the bw cut-over activities and initial source-bw system configurations during integration test cycles along with transport management.\n",
            "\n",
            "involved in performance tuning techniques of bw system like archiving, psa and change log tables deletion.\n",
            "\n",
            "worked closely with apo in demand planning and supply network planning\n",
            "\n",
            "involved in gap analysis to arrive at complete functional specifications and done the technical documentation\n",
            "chunk numer::  8\n",
            "worked on sap bo security setup, broadcasting and have worked through sap bobj webi.\n",
            "\n",
            "worked in hana remediation for emea region wherein we modulate the abap code to support the hana data base\n",
            "chunk numer::  9\n",
            "involved in hana upgradation project; on technical front; analyzed the upgradation issues and suggested the relevant notes, run the z program to see the object errors to ensure smoother system process, tweaked the codes to get in line with the hana system. on functional front, done with the front end and backed end tests across the landscape along with the data loads monitoring, loads validations, reports validation, issues fix during the load runs\n",
            "chunk numer::  10\n",
            "had worked through the odp framework for delta management to make sure the smother delta mechanism.\n",
            "\n",
            "worked through different ticketing tools like remedy, heat, service now etc. also, using the jira for agile scope projects to deliver in sprint wise.\n",
            "\n",
            "involved in a presale for a couple of projects.\n",
            "\n",
            "work history\n",
            "chunk numer::  11\n",
            "client duration delta faucet jun 2017 – present stanley black & decker mar 2016 - may 2017 general electric oct 2015 – feb 2016 hibu aug 2014 – sep 2015 akzonobel jan 2014 – jul 2014 provimi, essent, kpn and maxxium apr 2011 – dec 2013 maxxium jul 2010 – mar 2011\n",
            "\n",
            "professional experience\n",
            "\n",
            "client: delta faucet jun 2017 – present\n",
            "\n",
            "role: delivery lead & bw/4hana consultant\n",
            "\n",
            "responsibilities\n",
            "chunk numer::  12\n",
            "part of the pioneer green field implementation of s/4hana 1610 system.\n",
            "\n",
            "worked on the latest version of rebates management known as settlement management.\n",
            "\n",
            "enabled the new settlement management document settings in s/4 to enable the data feed to bw/4hana and designed the data flows with adsos, cps and open ods views then bex queries\n",
            "chunk numer::  13\n",
            "worked on bw/4hana 2.0 sp04 upgrade project throughout the landscape and applied a few notes during the project to get the existing objects worked. also, worked on s/4 2020 upgrade project throughout the landscape.\n",
            "\n",
            "prepared the technical specification documents and user test cases.\n",
            "\n",
            "for hypercare critical/high priority issues, have done hands-on system resolutions\n",
            "chunk numer::  14\n",
            "designed the plm and pm bi architecture; modelled the long-lasting design through which created critical business reports through ao as well as webi.\n",
            "\n",
            "designed the sales metrics, customer service metrics, inventory metrics, purchase metrics ad-hoc models and served the business with the critical reports.\n",
            "\n",
            "worked on the otc push reports; designed the logical model architecture using the available latest hana modelling objects like cds views, open ods views, cps and query as source\n",
            "chunk numer::  15\n",
            "leveraged the cds views functionality available with the relevant annotations. also, working through delta enabled cds views extractions.\n",
            "\n",
            "worked on the native hana modelling; created the views like attribute, analytic and calc views and done with the table functions using the existing hana models.\n",
            "\n",
            "created the procedures to get the data from third party systems using the sda.\n",
            "\n",
            "worked on dm/scm functional area analytics in association with legacy tool demantra\n",
            "chunk numer::  16\n",
            "involved in the data refresh pre and post activities to make sure the system health is good and smother.\n",
            "\n",
            "working on the performance tuning of the logical models for speedy reporting\n",
            "\n",
            "used the new global tables in s/4 like acdoca and prcd_elements tables for a few reporting needs and futuristic needs too. also, worked on the loading issues.\n",
            "\n",
            "worked on the odp framework introduced in s/4 for bi delta mechanism and working through odqmon tcode by using different options for fixing the load issues\n",
            "chunk numer::  17\n",
            "managing the new changes, requirement gatherings, fsd creations along with the design documentation.\n",
            "\n",
            "tracking the incidents and changes through service now to make sure the deliveries and upraising the team to get through the commitment.\n",
            "\n",
            "weekly status calls for incidents and changes and daily status calls for priority changes.\n",
            "\n",
            "analyzed the standard cds views introduced by sap and get them installed throughout the landscape\n",
            "chunk numer::  18\n",
            "recently worked on bo 4.3 upgrade project: get the smoke test activities done on webi as well as crystal reports, publications and webi job schedules across the landscape. also, fixed a couple of issues faced with publication and transportation.\n",
            "\n",
            "architected the powerbi expensive dashboard design; consuming hana expensive tables into views then dashboard at powerbi end\n",
            "chunk numer::  19\n",
            "recently solutioned the webi to powerbi conversion project wherein converting all the webi reports into powerbi reports via hana views; basically, creating hana views following best practices code push down having all the calculations done at hana views then, expose them to powerbi.\n",
            "\n",
            "solutioned the pbi designs per the existing data models; worked through dataflows, datasets, reports and paginated reports for high volume reports to have them downloaded hassle-free\n",
            "chunk numer::  20\n",
            "also, worked on the different kinds of dax functions during reports design and get the end to end functional and technical documentation.\n",
            "\n",
            "created a few dashboards for executives; executive dashboard, cas reports, sales details and plm dashboard. also, get automated daily, weekly, and monthly refreshments through power automate.\n",
            "\n",
            "helped the business with the a few paginated reports for huge volumes and have them scheduled to the network drives on regular basis\n",
            "chunk numer::  21\n",
            "client: stanley black & decker sep 2016 - may 2017\n",
            "\n",
            "role: lead sap bi consultant\n",
            "\n",
            "responsibilities:\n",
            "\n",
            "as part of hana upgradation, did the front end and backend tests across the landscape and documented the new features and prepared the test cases.\n",
            "\n",
            "done with the data load monitoring along with the different kinds of tests like records count check and kpis validation\n",
            "chunk numer::  22\n",
            "closely observed the entire phase of upgradation process that is being done through different activities like dmo process, pre-dmo steps, post-dmo steps and pca activity.\n",
            "\n",
            "fixed a few issues occurred during the data loads.\n",
            "\n",
            "tested the outbound files through apd and done the code fixes for the hardcoded system entries.\n",
            "\n",
            "participated in daily status calls.\n",
            "\n",
            "client: stanley black & decker jun 2016 – aug 2016\n",
            "\n",
            "role: lead sap bi consultant\n",
            "chunk numer::  23\n",
            "responsibilities:\n",
            "\n",
            "involved in changing the job schedules in qlikview.\n",
            "\n",
            "changed the date variable parameters by writing the scripts in qlikview.\n",
            "\n",
            "designed and built the other material types of inventories to generate the .qvd file in qlikview.\n",
            "\n",
            "created different variants for different regions to accommodate the requirement for the .qvd file generation\n",
            "chunk numer::  24\n",
            "analyzed the backend and frontend logics for global inventory projections, global inventory daily balances and weekly demand reports and prepared the details technical specification document for the business.\n",
            "\n",
            "run the bex variants around 140 for the global inventory projections history report for the qlikview requirement.\n",
            "\n",
            "worked with the canadian orderometer design change and did the logic changes.\n",
            "\n",
            "done with the data loads and validated the reports for their correctness\n",
            "chunk numer::  25\n",
            "analyzed the canadian orderometer design flow in detail and did the documentation.\n",
            "\n",
            "client: stanley black & decker mar 2016 – may 2016\n",
            "\n",
            "role: senior sap bi consultant\n",
            "\n",
            "responsibilities:\n",
            "\n",
            "done with the code remediation for the mapping rules like transfer rules, update rules, transformations.\n",
            "\n",
            "as part of the code remediation, worked with start, end and field routines extensively.\n",
            "\n",
            "prepared the test scripts for an uat\n",
            "chunk numer::  26\n",
            "prepared the unit and uat documents with after and before images of code.\n",
            "\n",
            "done with the data loads for before and after images code executions test.\n",
            "\n",
            "participated in daily update calls with the client.\n",
            "\n",
            "client: general electric oct 2015 – feb 2016\n",
            "\n",
            "role: senior sap bi consultant\n",
            "\n",
            "responsibilities:\n",
            "\n",
            "involved in requirement gathering.\n",
            "\n",
            "done with gap analysis and prepared the functional specification\n",
            "chunk numer::  27\n",
            "created the test data along with the ppm functional team.\n",
            "\n",
            "coordinating the team for the things (meetings with client, understanding the requirements, technical stuff) done in right way.\n",
            "\n",
            "created some generic extractors with views and function modules.\n",
            "\n",
            "insisting the team for good modeling so that system could be smoother even after lots of data.\n",
            "\n",
            "helping the team with critical tasks like coding part, converting the requirement into functional design and complex reports design\n",
            "chunk numer::  28\n",
            "insisting the team for well technical design so that it should be helpful as a further reference.\n",
            "\n",
            "client: hibu aug 2014 – sep 2015\n",
            "\n",
            "role: senior sap bi consultant\n",
            "\n",
            "responsibilities:\n",
            "\n",
            "support activities like monitoring the data loads and fixing the issue comes out on regular basis.\n",
            "\n",
            "weekly and monthly status meetings with onsite folks.\n",
            "\n",
            "working with third party system like nmbi\n",
            "chunk numer::  29\n",
            "involved in performance tuning.\n",
            "\n",
            "created different apds and ophds as per the business requirement.\n",
            "\n",
            "reconciling the sales data from r3 to bw.\n",
            "\n",
            "involved in cutover activities.\n",
            "\n",
            "created broadcasting reports through bex.\n",
            "\n",
            "client: akzonobel jan 2014 – jul 2014\n",
            "\n",
            "role: sap bi consultant\n",
            "\n",
            "responsibilities:\n",
            "\n",
            "client facing role by sitting in nl (5 weeks) in which i closely worked with our counter parts (smes)\n",
            "chunk numer::  30\n",
            "worked on bo security setup.\n",
            "\n",
            "worked on incidents, kcrs (service requests) and rfes (change requests).\n",
            "\n",
            "worked with alpha project which deals with query building based on the global business as well as local business requirements.\n",
            "\n",
            "given end-user-training on query building and report usage.\n",
            "\n",
            "worked with heat tool (ticketing tool).\n",
            "\n",
            "prepared knowledge acquisition plan (kap) based on which effectively taken the kt and also coordinated with other team members and smes for effective kt\n",
            "chunk numer::  31\n",
            "prepared bw aod based on the knowledge received from sme.\n",
            "\n",
            "involved in slo conversion wherein we have done controlling areas merge into single controlling area and involved in company code merging too.\n",
            "\n",
            "involved in creation of broadcasts through bex.\n",
            "\n",
            "attending weekly status updates calls.\n",
            "\n",
            "clients: provimi, essent, kpn and maxxium. apr 2011 – dec 2013\n",
            "\n",
            "roles: spoc, incident manager and bi consultant.\n",
            "\n",
            "responsibilities\n",
            "chunk numer::  32\n",
            "involved in support and maintenance work for a pool of different clients.\n",
            "\n",
            "worked on different roll-out projects like austria, panther, and spectra.\n",
            "\n",
            "as a part of austria, panther (singapore) and spectra (russia)rollouts.\n",
            "\n",
            "involved in the requirement gathering.\n",
            "\n",
            "prepared the fs and td documents.\n",
            "\n",
            "prepared the list of key reports.\n",
            "\n",
            "done required changes at data modeling level and report level as well.\n",
            "\n",
            "created the apds as per the requirements\n",
            "chunk numer::  33\n",
            "created a logical path and assigned the physical path.\n",
            "\n",
            "created the process chain to automate these apd programs triggering to place the file on the application server.\n",
            "\n",
            "created new report for stock comparison from r/3 system and warehouse.\n",
            "\n",
            "prepared the test cases, involved in uat support and issues resolved in uat support.\n",
            "\n",
            "prepared the cut-over activities and done with the transports along with after go-live support\n",
            "chunk numer::  34\n",
            "worked on early watch reports for different clients and implemented recommended action from sap.\n",
            "\n",
            "performed various tuning techniques (aggregates and partitioning) to optimize the query performance.\n",
            "\n",
            "in sap support, worked on various technical and functional issues in the form of tickets and change requests from different clients simultaneously.\n",
            "\n",
            "involved in performance issues like db statistics, indices, partitioning and compression of info cube, aggregates to improve the query response\n",
            "chunk numer::  35\n",
            "actively and regularly involved in load monitoring of daily, weekly and monthly, data loads using process chains and info packages.\n",
            "\n",
            "also involved in changing the existing partition range of cubes using repartitioning concepts.\n",
            "\n",
            "handled the tickets of priority 1, 2 and 3 by following (sop) standard operating procedures and never missed the (sla) service level agreement.\n",
            "\n",
            "dealing with the clients directly over phone on high priority tickets\n",
            "\n",
            "worked on 7.0 unit conversion model\n",
            "chunk numer::  36\n",
            "created manual hierarchy for one of markets as per their requirement.\n",
            "\n",
            "raised oss messages for critical issues.\n",
            "\n",
            "configured smtp, scot settings for email alert for process chains.\n",
            "\n",
            "data archiving in sap bw by using sara t-code.\n",
            "\n",
            "as an incident manager, was the responsibility for all the incidents routing through me for their timelines and had to make sure they met the deadlines and used to send the weekly status reports to management\n",
            "chunk numer::  37\n",
            "client: maxxium jul 2010 – mar 2011\n",
            "\n",
            "role: sap bi consultant\n",
            "\n",
            "responsibilities:\n",
            "\n",
            "involved in gap analysis\n",
            "\n",
            "extensively worked on lo extraction and done inits and delta loads\n",
            "\n",
            "worked on data source enhancements by using the enhancement rsap0001 to fill the data into the fields enhanced.\n",
            "\n",
            "worked on filling the setup tables and loaded data into bw.\n",
            "\n",
            "involved in creating generic extractors using function module and view\n",
            "chunk numer::  38\n",
            "as part of data provider layer migrated data from sap r/3 using lo, co-pa, business content extractors.\n",
            "\n",
            "unit testing in development box and integration testing in quality box.\n",
            "\n",
            "continuous status updating.\n",
            "\n",
            "developed the delivery checklist document and analyzed all the documents against that before submitting it to the clients.\n",
            "\n",
            "created the functional spec and technical specs too\n",
            "chunk numer::  39\n",
            "involved in improving the process by taking initiatives to improve the system performance, which involves maintaining db statistics, analyzing the utilization of olap statistics and taking effective decisions to improve the query performance.\n",
            "\n",
            "involved in debugging the update rules and transfer rules.\n",
            "\n",
            "extensively worked on installation and creation of info cubes, ods objects, info objects.\n",
            "\n",
            "worked on data loading by using the update methods initialize delta and full update\n",
            "chunk numer::  40\n",
            "worked on query designer in designing reports with conditions, exceptions, calculated key figures, restricted key figures, filters, and free characteristics.\n",
            "\n",
            "worked with rri to see the granular data.\n",
            "\n",
            "worked on creating and scheduling the process chains by using one of the process types decision step.\n",
            "\n",
            "data loading and load monitoring.\n",
            "\n",
            "educational background\n",
            "\n",
            "mba in marketing and finance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_chunks[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "sE78qh6M2I6J",
        "outputId": "1eba4c22-8d81-44a2-ea47-be657c9e90d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'led the design of a predictive regression model that increased daily foot traffic by 30% in a retail chain.\\n\\nbuilt and deployed rag-based genai applications using langchain, openai, and hugging face transformers, enabling context-aware responses from large unstructured datasets.\\n\\nexperience\\n\\nlead engineer | epam systems | 02/2025 – present\\n\\ndeveloped ai-enabled contract and rate management platform for healthcare providers'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx6jgfF30A60",
        "outputId": "0e2384da-64e9-4fa9-aa6b-a545203ae41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-4pq05M0fx-",
        "outputId": "1f04c5f8-275c-4c1e-e0e8-9d507d8b63f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'experience_years_total': 14,\n",
              " 'skills': ['SAP BW',\n",
              "  'SAP BI',\n",
              "  'ABAP',\n",
              "  'BW/4HANA',\n",
              "  'AMDP',\n",
              "  'Native HANA',\n",
              "  'BOBJ',\n",
              "  'Power BI',\n",
              "  'HANA Views',\n",
              "  'DAX',\n",
              "  'Power Automate',\n",
              "  'BEx Analyzer',\n",
              "  'BI Portal',\n",
              "  'APDs',\n",
              "  'QlikView',\n",
              "  'S/4HANA',\n",
              "  'Rebates Management',\n",
              "  'Settlement Management',\n",
              "  'PLM',\n",
              "  'PM',\n",
              "  'OTC',\n",
              "  'CDS Views',\n",
              "  'SDA',\n",
              "  'Demantra',\n",
              "  'ODP Framework',\n",
              "  'ServiceNow',\n",
              "  'JIRA',\n",
              "  'Remedy',\n",
              "  'Heat',\n",
              "  'SAP BO Security',\n",
              "  'Broadcasting',\n",
              "  'SAP BOBJ WebI',\n",
              "  'HANA Remediation',\n",
              "  'Ticketing Tools'],\n",
              " 'certifications': [],\n",
              " 'key_responsibilities': ['Solution architect in analytics practice',\n",
              "  'Solutioned webi to powerbi conversion project',\n",
              "  'Created dataflows, datasets, reports, paginated reports',\n",
              "  'Architected Power BI dashboard design',\n",
              "  'Worked on BO 4.3 upgrade project',\n",
              "  'Worked on BW/4HANA 1.0 green field implementation',\n",
              "  'Worked on BW/4HANA 2.0 upgrade projects',\n",
              "  'Worked extensively on SAP BI in various domains',\n",
              "  'Worked with ABAP CDS views and native HANA modeling',\n",
              "  'Provided key user training and proposed system improvements',\n",
              "  'Involved in performance tuning techniques',\n",
              "  'Worked on SAP BO security setup and broadcasting',\n",
              "  'Involved in HANA upgradation project',\n",
              "  'Worked through different ticketing tools',\n",
              "  'Involved in presale for projects',\n",
              "  'Part of pioneer green field implementation of S/4HANA 1610 system',\n",
              "  'Enabled new settlement management document settings in S/4',\n",
              "  'Designed PLM and PM BI architecture',\n",
              "  'Worked on OTC push reports',\n",
              "  'Leveraged CDS views functionality',\n",
              "  'Created procedures for third-party data integration',\n",
              "  'Involved in data refresh activities',\n",
              "  'Managed incidents and changes through ServiceNow',\n",
              "  'Analyzed standard CDS views introduced by SAP',\n",
              "  'Solutioned PBI designs per existing data models',\n",
              "  'Created executive dashboards',\n",
              "  'Helped business with paginated reports',\n",
              "  'Involved in requirement gathering and gap analysis',\n",
              "  'Created generic extractors with views and function modules',\n",
              "  'Involved in performance tuning and cutover activities',\n",
              "  'Worked on early watch reports and implemented recommended actions',\n",
              "  'Handled tickets and change requests',\n",
              "  'Involved in debugging and improving system performance']}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docx_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rcZuXr5l0nK7",
        "outputId": "2c813e09-71cf-4249-bb17-2ed7d0892ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/resume_db/Candidate 105.docx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start Weaviate Collection and Data Preparation**"
      ],
      "metadata": {
        "id": "74tj6riB0urT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install weaviate-client==4.16.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "ws5B07YV0031",
        "outputId": "cfc5b02f-0afc-453e-fccb-3763ff1ff75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting weaviate-client==4.16.6\n",
            "  Downloading weaviate_client-4.16.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (0.28.1)\n",
            "Collecting validators<1.0.0,>=0.34.0 (from weaviate-client==4.16.6)\n",
            "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (1.6.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (2.11.10)\n",
            "Requirement already satisfied: grpcio<1.80.0,>=1.59.5 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (1.75.1)\n",
            "Collecting grpcio-health-checking<1.80.0,>=1.59.5 (from weaviate-client==4.16.6)\n",
            "  Downloading grpcio_health_checking-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from weaviate-client==4.16.6)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client==4.16.6) (43.0.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation<3.0.0,>=2.1.0->weaviate-client==4.16.6) (25.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<1.80.0,>=1.59.5->weaviate-client==4.16.6) (4.15.0)\n",
            "Collecting protobuf<7.0.0,>=6.31.1 (from grpcio-health-checking<1.80.0,>=1.59.5->weaviate-client==4.16.6)\n",
            "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client==4.16.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client==4.16.6) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client==4.16.6) (0.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client==4.16.6) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client==4.16.6) (2.23)\n",
            "Downloading weaviate_client-4.16.6-py3-none-any.whl (597 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m597.5/597.5 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading grpcio_health_checking-1.75.1-py3-none-any.whl (18 kB)\n",
            "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: validators, protobuf, deprecation, grpcio-health-checking, weaviate-client\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deprecation-2.1.0 grpcio-health-checking-1.75.1 protobuf-6.33.0 validators-0.35.0 weaviate-client-4.16.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "50f5433cc75941a0b056d44e9a9fffa5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEAVIATE_URL = \"eq9xrs3jtoep5ksrao86bw.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
        "WEAVIATE_API_KEY = \"cGxDeERtK0ZUUkQ5Wkt1Z19rWDJlWHNxZ0dzNnlmbFZSV2QvS2k3TER6ZDV4NEVMb2U3dTd3dGRteStZPV92MjAw\"\n",
        "OPENAI_API_KEY = \"\"\n",
        "CollectionName = \"resume\""
      ],
      "metadata": {
        "id": "qVgXEY1l1f57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Weaviate Client"
      ],
      "metadata": {
        "id": "Reb-Q3S92R-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "import os\n",
        "\n",
        "headers = {\n",
        "    \"X-Openai-Api-Key\": OPENAI_API_KEY\n",
        "}\n",
        "\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=WEAVIATE_URL,  # Replace with your WCD URL\n",
        "    auth_credentials=WEAVIATE_API_KEY, # Replace with your WCD key\n",
        "    headers=headers,\n",
        ")"
      ],
      "metadata": {
        "id": "64grXjmi10Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Collection"
      ],
      "metadata": {
        "id": "3mJnGuj83AV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weaviate.classes.config import Property, DataType, Configure\n",
        "\n",
        "if not client.collections.exists(CollectionName):\n",
        "        client.collections.create(\n",
        "            name=CollectionName,\n",
        "            properties=[\n",
        "                Property(name=\"resume_link\", data_type=DataType.TEXT),\n",
        "                Property(name=\"key_responsibilities\", data_type=DataType.TEXT_ARRAY),\n",
        "                Property(name=\"certifications\", data_type=DataType.TEXT_ARRAY),\n",
        "                Property(name=\"skills\", data_type=DataType.TEXT_ARRAY),\n",
        "                Property(name=\"year_of_experince\", data_type=DataType.NUMBER),\n",
        "                Property(name=\"resume_chunk\", data_type=DataType.TEXT),\n",
        "            ],\n",
        "            vector_config=[\n",
        "                Configure.Vectors.text2vec_openai(\n",
        "                    name=\"key_responsibilities\",\n",
        "                    source_properties=[\"key_responsibilities\"],\n",
        "                    dimensions=512,\n",
        "                    model=\"text-embedding-3-small\",\n",
        "                ),\n",
        "                Configure.Vectors.text2vec_openai(\n",
        "                    name=\"resume\",\n",
        "                    source_properties=[\"resume_chunk\"],\n",
        "                    dimensions=512,\n",
        "                    model=\"text-embedding-3-small\",\n",
        "                ),\n",
        "                  Configure.Vectors.text2vec_openai(\n",
        "                    name=\"skills\",\n",
        "                    source_properties=[\"skills\",\"certifications\"],\n",
        "                    dimensions=512,\n",
        "                    model=\"text-embedding-3-small\",\n",
        "                )\n",
        "            ],\n",
        "        )\n",
        "    # END_SOLUTION\n",
        "else:\n",
        "        raise RuntimeError(\n",
        "            \"Collection 'RESUME' already exists! \"\n",
        "            \"If you like to re-build the collection, create and run a separate script to delete the existing collection. \"\n",
        "            \"\\n\\nTo delete a collection, run: client.collections.delete(<collection_name>)\"\n",
        "        )"
      ],
      "metadata": {
        "id": "brwMbtdD3K3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process each chunks and transform it into weaviate object and save to database**"
      ],
      "metadata": {
        "id": "XpjBTSqvmm5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resume_collection = client.collections.use(CollectionName)"
      ],
      "metadata": {
        "id": "x2on0-ODmwgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from weaviate.util import generate_uuid5\n",
        "\n",
        "# Enter context manager\n",
        "with resume_collection.batch.fixed_size(batch_size=200) as batch:\n",
        "\n",
        "    for chunk in tqdm(cleaned_chunks):\n",
        "        # Build the object payload\n",
        "        resume_obj = {\n",
        "            \"resume_link\": docx_path,\n",
        "            \"key_responsibilities\": [item.lower() for item in llm_response_json['key_responsibilities']],\n",
        "            \"certifications\":[item.lower() for item in llm_response_json['certifications']],\n",
        "            \"skills\": [item.lower() for item in llm_response_json['skills']],\n",
        "            \"year_of_experince\": llm_response_json['experience_years_total'],\n",
        "            \"resume_chunk\": chunk,\n",
        "        }\n",
        "\n",
        "        # Add object to batch queue\n",
        "        batch.add_object(\n",
        "            properties=resume_obj,\n",
        "            uuid=generate_uuid5(chunk)\n",
        "        )\n",
        "        # Batcher automatically sends batches\n",
        "\n",
        "# Check for failed objects\n",
        "if len(resume_collection.batch.failed_objects) > 0:\n",
        "    print(f\"Failed to import {len(resume_collection.batch.failed_objects)} objects\")\n",
        "\n",
        "client.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbb5H1u8nNBF",
        "outputId": "d3afe2c3-4bb1-49df-8cf4-dd20f6f2f85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41/41 [00:00<00:00, 12852.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.collections.delete(CollectionName)"
      ],
      "metadata": {
        "id": "Nve_10Shf1IA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}