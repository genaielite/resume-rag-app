{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8AxhaFfuLP_",
        "outputId": "01bed569-75aa-4b0f-8f67-e85c0ce0a39d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m665.6/981.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.5/80.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install -qU langchain-openai langchain langchain-community sentence-transformers unstructured[all-docs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "from unstructured.partition.docx import partition_docx\n",
        "from pathlib import Path\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "from unstructured.cleaners.core import bytes_string_to_string\n",
        "from unstructured.cleaners.core import clean\n",
        "from unstructured.cleaners.core import clean_bullets\n",
        "from unstructured.cleaners.core import clean_dashes\n",
        "from unstructured.cleaners.core import clean_non_ascii_chars\n",
        "from unstructured.cleaners.core import clean_ordered_bullets\n",
        "from unstructured.cleaners.core import clean_trailing_punctuation\n",
        "from unstructured.cleaners.core import remove_punctuation"
      ],
      "metadata": {
        "id": "Sk4xWjPlunVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load JD"
      ],
      "metadata": {
        "id": "xg2xQ69wpfn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pathlib import Path\n",
        "from docx import Document\n",
        "\n",
        "def load_jd(folder: str | Path, pattern: str = \"*.docx\") -> List[str]:\n",
        "    folder = Path(folder)\n",
        "    if not folder.exists():\n",
        "        raise FileNotFoundError(f\"Folder not found: {folder.resolve()}\")\n",
        "\n",
        "    jd_list = []\n",
        "    for file in sorted(folder.glob(pattern)):\n",
        "        if file.is_file():\n",
        "          file_path = str(file.absolute())\n",
        "          doc = Document(file_path)\n",
        "          jd_text = \"\"\n",
        "          for para in doc.paragraphs:\n",
        "            jd_text += para.text + \"\\n\"\n",
        "        jd_list.append(jd_text)\n",
        "    return jd_list"
      ],
      "metadata": {
        "id": "Oz-jx0CXph7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jd = load_jd(\"/content/jd\")"
      ],
      "metadata": {
        "id": "45OboNVRppna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jd_text = jd[0]\n",
        "print(jd_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq182fWR5N3n",
        "outputId": "8d91e299-d5db-4541-8ae2-65b3ba6942b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job Title:\n",
            "Solution Architect – SAP Analytics & BI\n",
            "Job Summary:\n",
            "We are seeking a highly experienced Solution Architect – Analytics with 14 years of proven expertise in SAP BW/BI, BW/4HANA, ABAP, AMDP, Native HANA, and Power BI. The ideal candidate will architect and implement end-to-end enterprise analytics solutions, integrating SAP and non-SAP systems to deliver scalable, high-performance reporting and business intelligence platforms. This role requires a deep understanding of HANA data modeling, Power BI dashboarding, and modern analytics frameworks, with strong experience in leading upgrades, migrations, and performance optimization projects.\n",
            "Key Responsibilities:\n",
            "Lead the design and implementation of enterprise analytics solutions using SAP BW/4HANA, SAP HANA, and Power BI.\n",
            "Architect and solution large-scale reporting platforms, including WEBI to Power BI conversions and BO 4.3 upgrades.\n",
            "Design, develop, and optimize HANA Views, ADSO, Composite Providers, Open ODS Views, and CDS Views to support analytical requirements.\n",
            "Build and automate Power BI dataflows, datasets, dashboards, and paginated reports using data from HANA, Azure Synapse, and SharePoint.\n",
            "Apply best practices in code pushdown and performance tuning to optimize data models and query execution.\n",
            "Work closely with business stakeholders to gather requirements, prepare technical specifications, and ensure successful delivery of BI solutions.\n",
            "Oversee data migrations, upgrades, and cut-over activities during SAP landscape transitions (e.g., BW to BW/4HANA, HANA upgrades).\n",
            "Manage incidents, track deliverables through ServiceNow/JIRA, and lead offshore teams through Agile delivery cycles.\n",
            "Provide technical leadership for performance tuning, report optimization, and system health monitoring.\n",
            "Collaborate with cross-functional teams in manufacturing, retail, utilities, and chemical domains to deliver end-to-end analytics solutions.\n",
            "Required Skills & Competencies:\n",
            "Strong expertise in SAP BW/4HANA, SAP HANA Modelling, ABAP CDS Views, AMDP, and BOBJ/Web Intelligence.\n",
            "Proven experience in Power BI architecture and dashboard design, including DAX, Power Automate, and data refresh automation.\n",
            "Hands-on experience with SAP BI data modeling (InfoObjects, ADSOs, Cubes, MultiProviders, InfoSets).\n",
            "In-depth knowledge of data extraction and integration using ODP, SDA, and CDS-based delta frameworks.\n",
            "Proficiency in functional areas such as Logistics, Purchasing, Materials Management, Finance, COPA, PM, and PLM.\n",
            "Strong experience with incident management, change management, and performance tuning.\n",
            "Excellent analytical, problem-solving, and communication skills with experience in client-facing roles.\n",
            "Preferred Qualifications / Nice-to-Have:\n",
            "Experience with Azure Synapse Analytics and cloud-based BI integration.\n",
            "Exposure to SAP S/4HANA Settlement Management and advanced analytics use cases.\n",
            "Familiarity with QlikView, Crystal Reports, and other BI tools.\n",
            "Knowledge of DevOps and Agile delivery methodologies.\n",
            "Experience working in multi-country SAP landscapes and upgrade environments.\n",
            "Experience Level:\n",
            "14+ years of experience in SAP BI/Analytics, including 5+ years as a Solution Architect.\n",
            "Location / Work Mode:\n",
            "Bangalore / Hyderabad / Mumbai – Hybrid / Flexible onsite engagement.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building JD Parser using LLM - Prompting"
      ],
      "metadata": {
        "id": "Qu3IdI-alPOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "CgGkGzgalWp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are a world-class information extraction system. Extract fields from a Job Description (JD) and return STRICT JSON only.\n",
        "\n",
        "### JSON SCHEMA (MANDATORY KEYS) - Below are the valid key and value of the JSON Object\n",
        "\n",
        "  \"job_title\": string,\n",
        "  \"job_summary\": string,\n",
        "  \"key_responsibilities\": string[],        // clean bullet points; one idea per item\n",
        "  \"required_skills\": string[],             // normalize tech/tool names; dedupe\n",
        "  \"qualifications\": string[],              // degrees, certifications, nice-to-have\n",
        "  \"experience\":                           // parse if present; else keep strings or nulls\n",
        "    \"total_years_min\": number|null,\n",
        "    \"total_years_max\": number|null,\n",
        "    \"notes\": string                        // original phrasing if ranges/mixed\n",
        "  ,\n",
        "  \"location\": string                       // City/State/Country + work mode if present\n",
        "\n",
        "\n",
        "### NORMALIZATION RULES\n",
        "- Preserve meaning, but clean text: remove leading bullets, numbering, and emojis.\n",
        "- Keep **lists** as arrays of short, declarative lines (no trailing punctuation).\n",
        "- Dedupe semantically identical items; prefer canonical tool names (e.g., \"PyTorch\", \"TensorFlow\").\n",
        "- If a field is missing in the JD, return an empty string `\"\"` or empty array `[]` as appropriate; keep keys.\n",
        "- Experience parsing:\n",
        "  - Extract numeric years where possible. If “12–15 years”, set `total_years_min=12`, `total_years_max=15`.\n",
        "  - If only one value is present (“14+ years”), set `min=14`, `max=null` and put the original phrase in `notes`.\n",
        "  - Always fill `notes` with the closest original text (e.g., “12–15 years total; 5+ years AI/ML; 2+ years GenAI”).\n",
        "- Location: include work mode if present (e.g., “Bangalore, India — Hybrid/Remote”).\n",
        "\n",
        "### OUTPUT\n",
        "- Return **ONLY** valid JSON (double quotes for all strings, no comments, no markdown fences).\n",
        "- Do not add extra keys.\n",
        "\n",
        "### INPUT JD\n",
        "{jd_text}\n",
        "\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "28KHaaYsnRxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(prompt_template)"
      ],
      "metadata": {
        "id": "aTb2vBSf4wvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2,max_tokens=700)\n",
        "chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "llm_response_json = chain.invoke({\"jd_text\":jd_text})\n",
        "llm_response_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw6nKWlL45xv",
        "outputId": "53b619e0-7c02-4bef-8cb5-7c106dbd446e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job_title': 'Solution Architect – SAP Analytics & BI',\n",
              " 'job_summary': 'We are seeking a highly experienced Solution Architect – Analytics with 14 years of proven expertise in SAP BW/BI, BW/4HANA, ABAP, AMDP, Native HANA, and Power BI. The ideal candidate will architect and implement end-to-end enterprise analytics solutions, integrating SAP and non-SAP systems to deliver scalable, high-performance reporting and business intelligence platforms. This role requires a deep understanding of HANA data modeling, Power BI dashboarding, and modern analytics frameworks, with strong experience in leading upgrades, migrations, and performance optimization projects.',\n",
              " 'key_responsibilities': ['Lead the design and implementation of enterprise analytics solutions using SAP BW/4HANA, SAP HANA, and Power BI',\n",
              "  'Architect and solution large-scale reporting platforms, including WEBI to Power BI conversions and BO 4.3 upgrades',\n",
              "  'Design, develop, and optimize HANA Views, ADSO, Composite Providers, Open ODS Views, and CDS Views to support analytical requirements',\n",
              "  'Build and automate Power BI dataflows, datasets, dashboards, and paginated reports using data from HANA, Azure Synapse, and SharePoint',\n",
              "  'Apply best practices in code pushdown and performance tuning to optimize data models and query execution',\n",
              "  'Work closely with business stakeholders to gather requirements, prepare technical specifications, and ensure successful delivery of BI solutions',\n",
              "  'Oversee data migrations, upgrades, and cut-over activities during SAP landscape transitions (e.g., BW to BW/4HANA, HANA upgrades)',\n",
              "  'Manage incidents, track deliverables through ServiceNow/JIRA, and lead offshore teams through Agile delivery cycles',\n",
              "  'Provide technical leadership for performance tuning, report optimization, and system health monitoring',\n",
              "  'Collaborate with cross-functional teams in manufacturing, retail, utilities, and chemical domains to deliver end-to-end analytics solutions'],\n",
              " 'required_skills': ['SAP BW/4HANA',\n",
              "  'SAP HANA Modelling',\n",
              "  'ABAP CDS Views',\n",
              "  'AMDP',\n",
              "  'BOBJ/Web Intelligence',\n",
              "  'Power BI',\n",
              "  'DAX',\n",
              "  'Power Automate',\n",
              "  'SAP BI data modeling',\n",
              "  'ODP',\n",
              "  'SDA',\n",
              "  'CDS-based delta frameworks',\n",
              "  'Logistics',\n",
              "  'Purchasing',\n",
              "  'Materials Management',\n",
              "  'Finance',\n",
              "  'COPA',\n",
              "  'PM',\n",
              "  'PLM',\n",
              "  'incident management',\n",
              "  'change management',\n",
              "  'performance tuning',\n",
              "  'analytical skills',\n",
              "  'problem-solving skills',\n",
              "  'communication skills'],\n",
              " 'qualifications': ['Experience with Azure Synapse Analytics and cloud-based BI integration',\n",
              "  'Exposure to SAP S/4HANA Settlement Management and advanced analytics use cases',\n",
              "  'Familiarity with QlikView, Crystal Reports, and other BI tools',\n",
              "  'Knowledge of DevOps and Agile delivery methodologies',\n",
              "  'Experience working in multi-country SAP landscapes and upgrade environments'],\n",
              " 'experience': {'total_years_min': 14,\n",
              "  'total_years_max': None,\n",
              "  'notes': '14+ years of experience in SAP BI/Analytics, including 5+ years as a Solution Architect'},\n",
              " 'location': 'Bangalore / Hyderabad / Mumbai – Hybrid / Flexible'}"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response_json['key_responsibilities']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfOokoVX6F-f",
        "outputId": "d06af1e6-1323-4b39-e0f9-68cff4ebf3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lead the design and implementation of enterprise analytics solutions using SAP BW/4HANA, SAP HANA, and Power BI',\n",
              " 'Architect and solution large-scale reporting platforms, including WEBI to Power BI conversions and BO 4.3 upgrades',\n",
              " 'Design, develop, and optimize HANA Views, ADSO, Composite Providers, Open ODS Views, and CDS Views to support analytical requirements',\n",
              " 'Build and automate Power BI dataflows, datasets, dashboards, and paginated reports using data from HANA, Azure Synapse, and SharePoint',\n",
              " 'Apply best practices in code pushdown and performance tuning to optimize data models and query execution',\n",
              " 'Work closely with business stakeholders to gather requirements, prepare technical specifications, and ensure successful delivery of BI solutions',\n",
              " 'Oversee data migrations, upgrades, and cut-over activities during SAP landscape transitions (e.g., BW to BW/4HANA, HANA upgrades)',\n",
              " 'Manage incidents, track deliverables through ServiceNow/JIRA, and lead offshore teams through Agile delivery cycles',\n",
              " 'Provide technical leadership for performance tuning, report optimization, and system health monitoring',\n",
              " 'Collaborate with cross-functional teams in manufacturing, retail, utilities, and chemical domains to deliver end-to-end analytics solutions']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEAVIATE_URL = \"eq9xrs3jtoep5ksrao86bw.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
        "WEAVIATE_API_KEY = \"cGxDeERtK0ZUUkQ5Wkt1Z19rWDJlWHNxZ0dzNnlmbFZSV2QvS2k3TER6ZDV4NEVMb2U3dTd3dGRteStZPV92MjAw\"\n",
        "OPENAI_API_KEY = \"\"\n",
        "CollectionName = \"resume\""
      ],
      "metadata": {
        "id": "KPb_kdGEJEDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reranker\n",
        "RERANKER_MODEL=\"BAAI/bge-reranker-base\"\n",
        "RERANK_TOP_K=3"
      ],
      "metadata": {
        "id": "j1I2HV4HR6eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Weaviate Client"
      ],
      "metadata": {
        "id": "uSxZ1rdfJKnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install weaviate-client==4.16.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "UYfRsTd2JQHo",
        "outputId": "55303c8d-cedd-4760-d3a5-2cceadadc3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weaviate-client==4.16.6 in /usr/local/lib/python3.12/dist-packages (4.16.6)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (0.28.1)\n",
            "Requirement already satisfied: validators<1.0.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (0.35.0)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (1.6.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (2.11.10)\n",
            "Requirement already satisfied: grpcio<1.80.0,>=1.59.5 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (1.75.1)\n",
            "Requirement already satisfied: grpcio-health-checking<1.80.0,>=1.59.5 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (1.75.1)\n",
            "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client==4.16.6) (2.1.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client==4.16.6) (43.0.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation<3.0.0,>=2.1.0->weaviate-client==4.16.6) (25.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<1.80.0,>=1.59.5->weaviate-client==4.16.6) (4.15.0)\n",
            "Collecting protobuf<7.0.0,>=6.31.1 (from grpcio-health-checking<1.80.0,>=1.59.5->weaviate-client==4.16.6)\n",
            "  Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client==4.16.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client==4.16.6) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client==4.16.6) (0.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client==4.16.6) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client==4.16.6) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client==4.16.6) (2.23)\n",
            "Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-6.33.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "a0b0d671413144cb8ff3df64ce9f3ea7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "import os\n",
        "\n",
        "headers = {\n",
        "    \"X-Openai-Api-Key\": OPENAI_API_KEY\n",
        "}\n",
        "\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=WEAVIATE_URL,  # Replace with your WCD URL\n",
        "    auth_credentials=WEAVIATE_API_KEY, # Replace with your WCD key\n",
        "    headers=headers,\n",
        ")"
      ],
      "metadata": {
        "id": "DbNKF6WQJIYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval By Key Responsibilities, Skills,Job Summary - Hybrid Search -> Rerank"
      ],
      "metadata": {
        "id": "cKIrdsKh6ihm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weaviate.classes.query import Filter\n",
        "\n",
        "min_exp = llm_response_json['experience']['total_years_min']\n",
        "max_exp = llm_response_json['experience']['total_years_max']\n",
        "\n",
        "filters = None\n",
        "if min_exp and max_exp:\n",
        "    filters = (\n",
        "        Filter.by_property(\"year_of_experince\").greater_or_equal(min_exp)\n",
        "        & Filter.by_property(\"year_of_experince\").less_or_equal(max_exp)\n",
        "    )\n",
        "elif min_exp:\n",
        "    filters = Filter.by_property(\"year_of_experince\").greater_or_equal(min_exp)\n",
        "elif max_exp:\n",
        "    filters = Filter.by_property(\"year_of_experince\").less_or_equal(max_exp)"
      ],
      "metadata": {
        "id": "MNZKj2gG6qgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Creating Query to find right chunk**"
      ],
      "metadata": {
        "id": "RQ09DwjwCgsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_by_responsibility = \"\"\n",
        "query_by_skills = \"\"\n",
        "query_by_job_summary = llm_response_json['job_summary'].lower()\n",
        "\n",
        "query1 = [responsibility.lower() for responsibility in llm_response_json['key_responsibilities']]\n",
        "query2 = [skills.lower() for skills in llm_response_json['required_skills']]\n",
        "\n",
        "for q in query1:\n",
        "  query_by_responsibility += f\" {q}\"\n",
        "\n",
        "for q in query2:\n",
        "  query_by_skills += f\" {q}\"\n"
      ],
      "metadata": {
        "id": "gmi6iYtVNDxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_by_job_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "zrtqa3BxNYBp",
        "outputId": "2e9c8a87-3e45-4c42-f28b-0f6f69e1a55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we are seeking a highly experienced solution architect – analytics with 14 years of proven expertise in sap bw/bi, bw/4hana, abap, amdp, native hana, and power bi. the ideal candidate will architect and implement end-to-end enterprise analytics solutions, integrating sap and non-sap systems to deliver scalable, high-performance reporting and business intelligence platforms. this role requires a deep understanding of hana data modeling, power bi dashboarding, and modern analytics frameworks, with strong experience in leading upgrades, migrations, and performance optimization projects.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Perform Hybrid Search with Weaviate db**"
      ],
      "metadata": {
        "id": "bsQ9lmAEEE-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resume = client.collections.use(CollectionName)\n",
        "\n",
        "response_for_responsibility = resume.query.hybrid(\n",
        "                query=query_by_responsibility,\n",
        "                filters=filters,\n",
        "                 alpha=0.5,\n",
        "                 limit=10,\n",
        "                target_vector=\"key_responsibilities\",\n",
        "            )\n",
        "\n",
        "response_for_skill = resume.query.hybrid(\n",
        "                query=query_by_skills,\n",
        "                filters=filters,\n",
        "                 alpha=0.5,\n",
        "                 limit=10,\n",
        "                target_vector=\"skills\",\n",
        "            )\n",
        "\n",
        "response_for_job_summary = resume.query.hybrid(\n",
        "                query=query_by_job_summary,\n",
        "                filters=filters,\n",
        "                 alpha=0.5,\n",
        "                 limit=10,\n",
        "                target_vector=\"resume\",\n",
        "            )"
      ],
      "metadata": {
        "id": "in85RuuhOwa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.close()"
      ],
      "metadata": {
        "id": "6xee69Y7ErY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_for_job_summary.objects"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYVJbIk1Pdb5",
        "outputId": "20cfbf15-7b02-49d2-ea1e-bbb333644d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Object(uuid=_WeaviateUUIDInt('c634ce31-530a-5165-855a-69a84d857092'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'professional summary\\n\\n14 years of experience in sap bw/bi/abap, bw/4hana, amdp, native hana, bobj and powerbi.\\n\\nworking as a solution architect in analytics practice.\\n\\nrecently solutioned the webi to powerbi conversion project wherein converting all the webi reports into powerbi reports via hana views; basically, creating hana views following best practices code push down having all the calculations done at hana views then, expose them to powerbi', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume'),\n",
              " Object(uuid=_WeaviateUUIDInt('2976dafd-ab71-5953-bec8-cf62d2b8024e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'working with gyansys infotech pvt. ltd., as a solution architect in analytics practice.\\n\\nworked extensively on sap bi in the logistics, purchasing, materials management, inventory, copa, accounts payable, account receivables, general ledger, ppm, pm and plm.\\n\\nhave been working through the abap cds views, adsos, cps, open ods views, query providers along with native hana modelling objects like analytical views, calculated views, table functions and store procedures', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume'),\n",
              " Object(uuid=_WeaviateUUIDInt('3e7f391b-922d-5189-8530-71da6d7fa1bc'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'involved in performance tuning.\\n\\ncreated different apds and ophds as per the business requirement.\\n\\nreconciling the sales data from r3 to bw.\\n\\ninvolved in cutover activities.\\n\\ncreated broadcasting reports through bex.\\n\\nclient: akzonobel jan 2014 – jul 2014\\n\\nrole: sap bi consultant\\n\\nresponsibilities:\\n\\nclient facing role by sitting in nl (5 weeks) in which i closely worked with our counter parts (smes)', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume'),\n",
              " Object(uuid=_WeaviateUUIDInt('ac2dbecf-f6c6-55e5-8905-39e8addb97c4'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'client: stanley black & decker sep 2016 - may 2017\\n\\nrole: lead sap bi consultant\\n\\nresponsibilities:\\n\\nas part of hana upgradation, did the front end and backend tests across the landscape and documented the new features and prepared the test cases.\\n\\ndone with the data load monitoring along with the different kinds of tests like records count check and kpis validation', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume'),\n",
              " Object(uuid=_WeaviateUUIDInt('99086a8e-9dcb-582b-93a5-26200bc1f8e8'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'created dataflows, datasets, reports, paginated reports consuming the sources from hana, sharepoint, and azure synapse. worked through the different dax formulas during the report design.\\n\\narchitected the powerbi expensive dashboard design; consuming hana expensive tables into views then dashboard at powerbi end. automated the flows and datasets through power automate', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume'),\n",
              " Object(uuid=_WeaviateUUIDInt('0f539e89-478c-507b-b2dc-57879b42b650'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'worked on sap bo security setup, broadcasting and have worked through sap bobj webi.\\n\\nworked in hana remediation for emea region wherein we modulate the abap code to support the hana data base', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume'),\n",
              " Object(uuid=_WeaviateUUIDInt('85b69c34-ec26-58de-9a69-b415912cae1a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'insisting the team for well technical design so that it should be helpful as a further reference.\\n\\nclient: hibu aug 2014 – sep 2015\\n\\nrole: senior sap bi consultant\\n\\nresponsibilities:\\n\\nsupport activities like monitoring the data loads and fixing the issue comes out on regular basis.\\n\\nweekly and monthly status meetings with onsite folks.\\n\\nworking with third party system like nmbi', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume'),\n",
              " Object(uuid=_WeaviateUUIDInt('ccfe1dd5-f1dd-5625-8e65-f2224e2a8739'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'client: maxxium jul 2010 – mar 2011\\n\\nrole: sap bi consultant\\n\\nresponsibilities:\\n\\ninvolved in gap analysis\\n\\nextensively worked on lo extraction and done inits and delta loads\\n\\nworked on data source enhancements by using the enhancement rsap0001 to fill the data into the fields enhanced.\\n\\nworked on filling the setup tables and loaded data into bw.\\n\\ninvolved in creating generic extractors using function module and view', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume'),\n",
              " Object(uuid=_WeaviateUUIDInt('2e3259e3-4fe9-5ccd-a762-be4b61e54e54'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'leveraged the cds views functionality available with the relevant annotations. also, working through delta enabled cds views extractions.\\n\\nworked on the native hana modelling; created the views like attribute, analytic and calc views and done with the table functions using the existing hana models.\\n\\ncreated the procedures to get the data from third party systems using the sda.\\n\\nworked on dm/scm functional area analytics in association with legacy tool demantra', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume'),\n",
              " Object(uuid=_WeaviateUUIDInt('9f4743fb-87a3-500d-9255-dfcb6b1980bf'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'year_of_experince': 14.0, 'certifications': [], 'key_responsibilities': ['solution architect in analytics practice', 'solutioned webi to powerbi conversion project', 'created dataflows, datasets, reports, paginated reports', 'architected power bi dashboard design', 'worked on bo 4.3 upgrade project', 'worked on bw/4hana 1.0 green field implementation', 'worked on bw/4hana 2.0 upgrade projects', 'worked extensively on sap bi in various domains', 'worked with abap cds views and native hana modeling', 'provided key user training and proposed system improvements', 'involved in performance tuning techniques', 'worked on sap bo security setup and broadcasting', 'involved in hana upgradation project', 'worked through different ticketing tools', 'involved in presale for projects', 'part of pioneer green field implementation of s/4hana 1610 system', 'enabled new settlement management document settings in s/4', 'designed plm and pm bi architecture', 'worked on otc push reports', 'leveraged cds views functionality', 'created procedures for third-party data integration', 'involved in data refresh activities', 'managed incidents and changes through servicenow', 'analyzed standard cds views introduced by sap', 'solutioned pbi designs per existing data models', 'created executive dashboards', 'helped business with paginated reports', 'involved in requirement gathering and gap analysis', 'created generic extractors with views and function modules', 'involved in performance tuning and cutover activities', 'worked on early watch reports and implemented recommended actions', 'handled tickets and change requests', 'involved in debugging and improving system performance'], 'resume_chunk': 'recently solutioned the webi to powerbi conversion project wherein converting all the webi reports into powerbi reports via hana views; basically, creating hana views following best practices code push down having all the calculations done at hana views then, expose them to powerbi.\\n\\nsolutioned the pbi designs per the existing data models; worked through dataflows, datasets, reports and paginated reports for high volume reports to have them downloaded hassle-free', 'skills': ['sap bw', 'sap bi', 'abap', 'bw/4hana', 'amdp', 'native hana', 'bobj', 'power bi', 'hana views', 'dax', 'power automate', 'bex analyzer', 'bi portal', 'apds', 'qlikview', 's/4hana', 'rebates management', 'settlement management', 'plm', 'pm', 'otc', 'cds views', 'sda', 'demantra', 'odp framework', 'servicenow', 'jira', 'remedy', 'heat', 'sap bo security', 'broadcasting', 'sap bobj webi', 'hana remediation', 'ticketing tools'], 'resume_link': '/content/resume_db/Candidate 105.docx'}, references=None, vector={}, collection='Resume')]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Create Candidate chunks for Reranking**"
      ],
      "metadata": {
        "id": "vfyo3SdFTvFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_chunks_for_responsibility = [o for o in response_for_responsibility.objects]\n",
        "all_chunks_for_skills = [o for o in response_for_skill.objects]\n",
        "all_chunks_for_job_summary = [o for o in response_for_job_summary.objects]"
      ],
      "metadata": {
        "id": "h6W-gosLT2NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all_chunks"
      ],
      "metadata": {
        "id": "PFDVpv-kUHjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReRank Util Function**"
      ],
      "metadata": {
        "id": "SNNfnHnYSPuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.3 --force-reinstall --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-r5XqFiSkTA",
        "outputId": "f67a7964-3e93-4722-831d-35c731ddcd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-health-checking 1.75.1 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "onnx 1.19.1 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "from sentence_transformers import CrossEncoder"
      ],
      "metadata": {
        "id": "7eJt2Rf_SLdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "def rerank(query: str, candidates: List, top_k: int = None) -> List[Dict]:\n",
        "   _reranker = CrossEncoder(RERANKER_MODEL, trust_remote_code=True)\n",
        "\n",
        "   if top_k is None:\n",
        "        top_k = RERANK_TOP_K\n",
        "   if not candidates:\n",
        "        return []\n",
        "\n",
        "   # Convert Weaviate Objects to dictionaries for mutability\n",
        "   mutable_candidates = [c.properties for c in candidates]\n",
        "   pairs = [(query, c[\"resume_chunk\"]) for c in mutable_candidates]\n",
        "   scores = _reranker.predict(pairs)\n",
        "\n",
        "   for c, s in zip(mutable_candidates, scores):\n",
        "        c[\"rerank_score\"] = float(s)\n",
        "   ranked = sorted(mutable_candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "   return ranked[:top_k]"
      ],
      "metadata": {
        "id": "2voBuT9XSt1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling rerank function - for query and list of chunks"
      ],
      "metadata": {
        "id": "2OQR00aaUUNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rarank_chunks_for_responsibility = rerank(query_by_responsibility, all_chunks_for_responsibility)\n",
        "rarank_chunks_for_skill = rerank(query_by_skills, all_chunks_for_skills)\n",
        "rarank_chunks_for_job_summary = rerank(query_by_job_summary, all_chunks_for_job_summary)"
      ],
      "metadata": {
        "id": "t0zwTUa3UagZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(rarank_chunks_for_job_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZxCPHtgVvwn",
        "outputId": "5f5a04f5-4370-4160-eeac-79032a6c33e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine all chunks and find unique resume Link - This is the desired output/result**"
      ],
      "metadata": {
        "id": "Oncrh96tHSuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resume_link_set = set()\n",
        "\n",
        "for chunk in rarank_chunks_for_responsibility:\n",
        "  resume_link_set.add(chunk['resume_link'])\n",
        "\n",
        "for chunk in rarank_chunks_for_skill:\n",
        "  resume_link_set.add(chunk['resume_link'])\n",
        "\n",
        "for chunk in rarank_chunks_for_job_summary:\n",
        "  resume_link_set.add(chunk['resume_link'])"
      ],
      "metadata": {
        "id": "8s0SzcaVHLfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_link_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPkDEK0DHrAU",
        "outputId": "800de5f5-f5ff-4fb4-da86-2f1a0bf0fb01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/resume_db/Candidate 105.docx'}"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rarank_chunks_for_skill:\n",
        "  print(chunk['rerank_score'])\n",
        "  print(chunk['resume_chunk'])\n",
        "  print(chunk['resume_link'])\n",
        "  print(\"---------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXrhu6RuWVCJ",
        "outputId": "c8ebd3c7-5498-4ca4-f732-de91c8752912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7962647080421448\n",
            "developed finance and insurance web apps using java stack.\n",
            "\n",
            "education\n",
            "\n",
            "mca (master of computer applications), university of calcutta, 2007–2010\n",
            "\n",
            "skills\n",
            "\n",
            "python, pandas, numpy, scikit-learn, mlflow, tensorflow, pytorch, langchain, generative ai, prompt engineering, vector databases (pinecone), fastapi, docker, kubernetes, aws, gcp, sql, mongodb\n",
            "\n",
            "key strengths\n",
            "\n",
            "architectural mindset | emerging tech enthusiast | mentor & leader | innovation-driven mindset\n",
            "/content/resume_db/Rajesh_Kumar_Kushwaha_Resume.docx\n",
            "---------------------------------------------------\n",
            "0.5987187623977661\n",
            "built rag, langchain, vector db (pinecone) solution improving retrieval performance by 30%.\n",
            "\n",
            "data scientist | coforge | 08/2023 – 06/2024\n",
            "\n",
            "automated claim processing with nlp and ml models.\n",
            "\n",
            "fine-tuned bert for ner to extract policy information from claim forms.\n",
            "\n",
            "deployed ai pipeline using fastapi.\n",
            "\n",
            "data scientist | mercedes-benz | 11/2022 – 05/2023\n",
            "\n",
            "led dealer retention projects, improving engagement by 25%.\n",
            "\n",
            "developed random forest model predicting dealer dropouts\n",
            "/content/resume_db/Rajesh_Kumar_Kushwaha_Resume.docx\n",
            "---------------------------------------------------\n",
            "0.2071634829044342\n",
            "experienced data science professional with 14 years of hands-on expertise in developing scalable, end-to-end ai/ml and generative ai solutions across domains like insurance, retail, and manufacturing.\n",
            "\n",
            "proven success in applying advanced machine learning, deep learning, and large language models (llms) to solve real-world business problems.\n",
            "\n",
            "achieved a 15% boost in nlp model accuracy by fine-tuning bert on domain-specific claim data\n",
            "/content/resume_db/Rajesh_Kumar_Kushwaha_Resume.docx\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}